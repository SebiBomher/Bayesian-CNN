{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import tensorflow_probability as tfp\n",
    "import sklearn.metrics as metrics\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True  \n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "set_session(sess)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'fruits-360_dataset/fruits-360/Training'\n",
    "validation_data_dir = 'fruits-360_dataset/fruits-360/Test'\n",
    "test_data_dir = 'fruits-360_dataset/fruits-360/Test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 100, 100\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sebi\\.conda\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_probability\\python\\layers\\util.py:106: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Sebi\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_reparameterization (C (None, 100, 100, 16)      400       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 50, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_reparameterization_1  (None, 50, 50, 32)        4128      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 25, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_reparameterization_2  (None, 25, 25, 64)        16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_reparameterization_3  (None, 13, 13, 128)       65664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_flipout (DenseFlipout) (None, 180)               2258100   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               21720     \n",
      "=================================================================\n",
      "Total params: 2,366,460\n",
      "Trainable params: 2,366,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "modelBayes = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=input_shape),\n",
    "    tfp.layers.Convolution2DReparameterization(16, kernel_size = 2,  padding=\"SAME\", activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=2,  padding=\"SAME\"),\n",
    "    \n",
    "    tfp.layers.Convolution2DReparameterization(32, kernel_size = 2,  padding=\"SAME\", activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=2,  padding=\"SAME\"),\n",
    "    \n",
    "    tfp.layers.Convolution2DReparameterization(64, kernel_size = 2,  padding=\"SAME\", activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=2,  padding=\"SAME\"),\n",
    "    \n",
    "    tfp.layers.Convolution2DReparameterization(128, kernel_size = 2,  padding=\"SAME\", activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=2,  padding=\"SAME\"),\n",
    "    \n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tfp.layers.DenseFlipout(180, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(120)\n",
    "])\n",
    "print(modelBayes.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 100, 100, 16)      208       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 100, 100, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 50, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 50, 50, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 25, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 25, 25, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 180)               829620    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               21720     \n",
      "=================================================================\n",
      "Total params: 894,780\n",
      "Trainable params: 894,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 16, kernel_size = 2,input_shape=input_shape,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters = 32,kernel_size = 2,activation= 'relu',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters = 64,kernel_size = 2,activation= 'relu',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters = 128,kernel_size = 2,activation= 'relu',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(180))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(120,activation = 'softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "modelBayes.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print('Compiled!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48431 images belonging to 120 classes.\n",
      "Found 12067 images belonging to 120 classes.\n",
      "Found 20622 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs=30\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-77894e0200c3>:9: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 1.6568 - accuracy: 0.5428Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:35 - loss: 0.4117 - accuracy: 0.8970\n",
      "Epoch 00001: val_loss improved from inf to 0.41169, saving model to CNN_Multiclass.hdf5\n",
      "1513/1513 [==============================] - 333s 220ms/step - loss: 1.6560 - accuracy: 0.5430 - val_loss: 0.4117 - val_accuracy: 0.8970\n",
      "Epoch 2/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.3546 - accuracy: 0.8818Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:27 - loss: 0.3346 - accuracy: 0.9426\n",
      "Epoch 00002: val_loss improved from 0.41169 to 0.33459, saving model to CNN_Multiclass.hdf5\n",
      "1513/1513 [==============================] - 278s 184ms/step - loss: 0.3544 - accuracy: 0.8818 - val_loss: 0.3346 - val_accuracy: 0.9426\n",
      "Epoch 3/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.2066 - accuracy: 0.9326Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:45 - loss: 0.2778 - accuracy: 0.9359\n",
      "Epoch 00003: val_loss improved from 0.33459 to 0.27777, saving model to CNN_Multiclass.hdf5\n",
      "1513/1513 [==============================] - 260s 172ms/step - loss: 0.2066 - accuracy: 0.9326 - val_loss: 0.2778 - val_accuracy: 0.9359\n",
      "Epoch 4/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.1584 - accuracy: 0.9484Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:18 - loss: 0.2803 - accuracy: 0.9471\n",
      "Epoch 00004: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 249s 165ms/step - loss: 0.1583 - accuracy: 0.9484 - val_loss: 0.2803 - val_accuracy: 0.9471\n",
      "Epoch 5/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.1320 - accuracy: 0.9583Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:09 - loss: 0.3686 - accuracy: 0.9500\n",
      "Epoch 00005: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 225s 149ms/step - loss: 0.1321 - accuracy: 0.9583 - val_loss: 0.3686 - val_accuracy: 0.9500\n",
      "Epoch 6/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 0.9657Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:16 - loss: 0.3822 - accuracy: 0.9430\n",
      "Epoch 00006: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 226s 150ms/step - loss: 0.1174 - accuracy: 0.9657 - val_loss: 0.3822 - val_accuracy: 0.9430\n",
      "Epoch 7/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.1149 - accuracy: 0.9663Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:13 - loss: 0.3188 - accuracy: 0.9605\n",
      "Epoch 00007: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 227s 150ms/step - loss: 0.1149 - accuracy: 0.9663 - val_loss: 0.3188 - val_accuracy: 0.9605\n",
      "Epoch 8/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9662Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:11 - loss: 0.3489 - accuracy: 0.9593\n",
      "Epoch 00008: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 229s 151ms/step - loss: 0.1230 - accuracy: 0.9662 - val_loss: 0.3489 - val_accuracy: 0.9593\n",
      "Epoch 9/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9668Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:11 - loss: 0.3096 - accuracy: 0.9623\n",
      "Epoch 00009: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 229s 152ms/step - loss: 0.1235 - accuracy: 0.9668 - val_loss: 0.3096 - val_accuracy: 0.9623\n",
      "Epoch 10/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.1293 - accuracy: 0.9678Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:15 - loss: 0.4863 - accuracy: 0.9561\n",
      "Epoch 00010: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 233s 154ms/step - loss: 0.1295 - accuracy: 0.9678 - val_loss: 0.4863 - val_accuracy: 0.9561\n",
      "Epoch 11/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.1382 - accuracy: 0.9667Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:18 - loss: 0.3436 - accuracy: 0.9633\n",
      "Epoch 00011: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 235s 155ms/step - loss: 0.1382 - accuracy: 0.9667 - val_loss: 0.3436 - val_accuracy: 0.9633\n",
      "Epoch 12/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.1390 - accuracy: 0.9669Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:18 - loss: 0.4138 - accuracy: 0.9675\n",
      "Epoch 00012: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 236s 156ms/step - loss: 0.1389 - accuracy: 0.9669 - val_loss: 0.4138 - val_accuracy: 0.9675\n",
      "Epoch 13/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.1545 - accuracy: 0.9646Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:13 - loss: 0.4506 - accuracy: 0.9604\n",
      "Epoch 00013: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 232s 153ms/step - loss: 0.1544 - accuracy: 0.9646 - val_loss: 0.4506 - val_accuracy: 0.9604\n",
      "Epoch 14/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.1549 - accuracy: 0.9663Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:14 - loss: 0.5644 - accuracy: 0.9582\n",
      "Epoch 00014: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 232s 153ms/step - loss: 0.1548 - accuracy: 0.9663 - val_loss: 0.5644 - val_accuracy: 0.9582\n",
      "Epoch 15/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.1505 - accuracy: 0.9672Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:15 - loss: 0.4447 - accuracy: 0.9605\n",
      "Epoch 00015: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 235s 156ms/step - loss: 0.1504 - accuracy: 0.9672 - val_loss: 0.4447 - val_accuracy: 0.9605\n",
      "Epoch 16/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.1804 - accuracy: 0.9644Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:16 - loss: 0.4709 - accuracy: 0.9552\n",
      "Epoch 00016: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 236s 156ms/step - loss: 0.1803 - accuracy: 0.9644 - val_loss: 0.4709 - val_accuracy: 0.9552\n",
      "Epoch 17/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.1836 - accuracy: 0.9653Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:15 - loss: 0.4333 - accuracy: 0.9605\n",
      "Epoch 00017: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 237s 157ms/step - loss: 0.1835 - accuracy: 0.9654 - val_loss: 0.4333 - val_accuracy: 0.9605\n",
      "Epoch 18/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.2058 - accuracy: 0.9630Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:16 - loss: 1.0885 - accuracy: 0.9368\n",
      "Epoch 00018: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 232s 153ms/step - loss: 0.2059 - accuracy: 0.9630 - val_loss: 1.0885 - val_accuracy: 0.9368\n",
      "Epoch 19/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.2037 - accuracy: 0.9643Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:54 - loss: 0.6347 - accuracy: 0.9606\n",
      "Epoch 00019: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 216s 143ms/step - loss: 0.2036 - accuracy: 0.9643 - val_loss: 0.6347 - val_accuracy: 0.9606\n",
      "Epoch 20/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.2081 - accuracy: 0.9653Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:57 - loss: 0.7716 - accuracy: 0.9715\n",
      "Epoch 00020: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 193s 127ms/step - loss: 0.2080 - accuracy: 0.9653 - val_loss: 0.7716 - val_accuracy: 0.9715\n",
      "Epoch 21/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.2332 - accuracy: 0.9652Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:44 - loss: 0.3811 - accuracy: 0.9566\n",
      "Epoch 00021: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 185s 122ms/step - loss: 0.2335 - accuracy: 0.9652 - val_loss: 0.3811 - val_accuracy: 0.9566\n",
      "Epoch 22/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.2215 - accuracy: 0.9667 ETA: 0s - loss: 0.2220 - accuracyEpoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:46 - loss: 0.3903 - accuracy: 0.9677\n",
      "Epoch 00022: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 184s 122ms/step - loss: 0.2214 - accuracy: 0.9667 - val_loss: 0.3903 - val_accuracy: 0.9677\n",
      "Epoch 23/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.2408 - accuracy: 0.9633Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:50 - loss: 0.4603 - accuracy: 0.9541\n",
      "Epoch 00023: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 185s 123ms/step - loss: 0.2408 - accuracy: 0.9633 - val_loss: 0.4603 - val_accuracy: 0.9541\n",
      "Epoch 24/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.2519 - accuracy: 0.9649Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:47 - loss: 0.7023 - accuracy: 0.9641\n",
      "Epoch 00024: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 189s 125ms/step - loss: 0.2521 - accuracy: 0.9649 - val_loss: 0.7023 - val_accuracy: 0.9641\n",
      "Epoch 25/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.2539 - accuracy: 0.9647Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:44 - loss: 0.8103 - accuracy: 0.9717\n",
      "Epoch 00025: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 194s 128ms/step - loss: 0.2537 - accuracy: 0.9647 - val_loss: 0.8103 - val_accuracy: 0.9717\n",
      "Epoch 26/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.2710 - accuracy: 0.9635Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:46 - loss: 0.4266 - accuracy: 0.9607\n",
      "Epoch 00026: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 183s 121ms/step - loss: 0.2721 - accuracy: 0.9635 - val_loss: 0.4266 - val_accuracy: 0.9607\n",
      "Epoch 27/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.2859 - accuracy: 0.9615Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:49 - loss: 0.6510 - accuracy: 0.9595\n",
      "Epoch 00027: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 183s 121ms/step - loss: 0.2857 - accuracy: 0.9615 - val_loss: 0.6510 - val_accuracy: 0.9595\n",
      "Epoch 28/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.3105 - accuracy: 0.9640Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:57 - loss: 1.1205 - accuracy: 0.9568\n",
      "Epoch 00028: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 193s 127ms/step - loss: 0.3108 - accuracy: 0.9640 - val_loss: 1.1205 - val_accuracy: 0.9568\n",
      "Epoch 29/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.9648Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:45 - loss: 1.2943 - accuracy: 0.9572\n",
      "Epoch 00029: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 185s 122ms/step - loss: 0.2901 - accuracy: 0.9648 - val_loss: 1.2943 - val_accuracy: 0.9572\n",
      "Epoch 30/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 0.3053 - accuracy: 0.9659Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:46 - loss: 1.6410 - accuracy: 0.9353 ETA: 1:48 - los\n",
      "Epoch 00030: val_loss did not improve from 0.27777\n",
      "1513/1513 [==============================] - 182s 120ms/step - loss: 0.3053 - accuracy: 0.9659 - val_loss: 1.6410 - val_accuracy: 0.9353\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath = 'CNN_Multiclass.hdf5', verbose = 1, save_best_only = True)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks = [checkpointer],\n",
    "    validation_steps=validation_generator.samples // batch_size)\n",
    "\n",
    "model.save_weights('CNN_Multiclass-Final.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 2108532.3092 - accuracy: 0.0095 ETA: 1s - loss: 2114976Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:43 - loss: 1295370.1081 - accuracy: 0.0122\n",
      "Epoch 00001: val_loss improved from inf to 1295370.10809, saving model to Bayesyian_CNN_Multiclass.hdf5\n",
      "1513/1513 [==============================] - 187s 123ms/step - loss: 2107995.4206 - accuracy: 0.0095 - val_loss: 1295370.1081 - val_accuracy: 0.0122\n",
      "Epoch 2/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 632857.0483 - accuracy: 0.0099Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:47 - loss: 117261.6868 - accuracy: 0.0119\n",
      "Epoch 00002: val_loss improved from 1295370.10809 to 117261.68682, saving model to Bayesyian_CNN_Multiclass.hdf5\n",
      "1513/1513 [==============================] - 194s 128ms/step - loss: 632516.5067 - accuracy: 0.0099 - val_loss: 117261.6868 - val_accuracy: 0.0119\n",
      "Epoch 3/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 14950.1219 - accuracy: 0.0117Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:48 - loss: 8.3118 - accuracy: 0.0114\n",
      "Epoch 00003: val_loss improved from 117261.68682 to 8.31178, saving model to Bayesyian_CNN_Multiclass.hdf5\n",
      "1513/1513 [==============================] - 184s 122ms/step - loss: 14940.2501 - accuracy: 0.0117 - val_loss: 8.3118 - val_accuracy: 0.0114\n",
      "Epoch 4/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 8.3913 - accuracy: 0.0111Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:50 - loss: 8.3666 - accuracy: 0.0131\n",
      "Epoch 00004: val_loss did not improve from 8.31178\n",
      "1513/1513 [==============================] - 182s 120ms/step - loss: 8.3933 - accuracy: 0.0111 - val_loss: 8.3666 - val_accuracy: 0.0131\n",
      "Epoch 5/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 8.3408 - accuracy: 0.0108Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 2:00 - loss: 8.2819 - accuracy: 0.0104\n",
      "Epoch 00005: val_loss improved from 8.31178 to 8.28194, saving model to Bayesyian_CNN_Multiclass.hdf5\n",
      "1513/1513 [==============================] - 192s 127ms/step - loss: 8.3408 - accuracy: 0.0108 - val_loss: 8.2819 - val_accuracy: 0.0104\n",
      "Epoch 6/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 8.3384 - accuracy: 0.0110 ETA: 1s - loss: 8.3419 - accuracy: 0. - ETA: 1s -Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:42 - loss: 8.3222 - accuracy: 0.0131 ETA: 1:43 - loss: 8.3128 - accura\n",
      "Epoch 00006: val_loss did not improve from 8.28194\n",
      "1513/1513 [==============================] - 180s 119ms/step - loss: 8.3381 - accuracy: 0.0110 - val_loss: 8.3222 - val_accuracy: 0.0131\n",
      "Epoch 7/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 8.3972 - accuracy: 0.0115Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:43 - loss: 8.3434 - accuracy: 0.0111\n",
      "Epoch 00007: val_loss did not improve from 8.28194\n",
      "1513/1513 [==============================] - 177s 117ms/step - loss: 8.3968 - accuracy: 0.0115 - val_loss: 8.3434 - val_accuracy: 0.0111\n",
      "Epoch 8/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 8.3083 - accuracy: 0.0116Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:58 - loss: 8.4023 - accuracy: 0.0125\n",
      "Epoch 00008: val_loss did not improve from 8.28194\n",
      "1513/1513 [==============================] - 182s 120ms/step - loss: 8.3107 - accuracy: 0.0116 - val_loss: 8.4023 - val_accuracy: 0.0125\n",
      "Epoch 9/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 8.3712 - accuracy: 0.0111Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:43 - loss: 8.2753 - accuracy: 0.0128\n",
      "Epoch 00009: val_loss improved from 8.28194 to 8.27530, saving model to Bayesyian_CNN_Multiclass.hdf5\n",
      "1513/1513 [==============================] - 182s 120ms/step - loss: 8.3718 - accuracy: 0.0111 - val_loss: 8.2753 - val_accuracy: 0.0128\n",
      "Epoch 10/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 8.3842 - accuracy: 0.0121Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:41 - loss: 8.3793 - accuracy: 0.0127\n",
      "Epoch 00010: val_loss did not improve from 8.27530\n",
      "1513/1513 [==============================] - 187s 123ms/step - loss: 8.3825 - accuracy: 0.0121 - val_loss: 8.3793 - val_accuracy: 0.0127\n",
      "Epoch 11/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 8.3030 - accuracy: 0.0109Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:41 - loss: 8.2065 - accuracy: 0.0138\n",
      "Epoch 00011: val_loss improved from 8.27530 to 8.20651, saving model to Bayesyian_CNN_Multiclass.hdf5\n",
      "1513/1513 [==============================] - 177s 117ms/step - loss: 8.3020 - accuracy: 0.0109 - val_loss: 8.2065 - val_accuracy: 0.0138\n",
      "Epoch 12/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 8.2636 - accuracy: 0.0112Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:55 - loss: 8.3709 - accuracy: 0.0129\n",
      "Epoch 00012: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 180s 119ms/step - loss: 8.2647 - accuracy: 0.0112 - val_loss: 8.3709 - val_accuracy: 0.0129\n",
      "Epoch 13/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 8.3705 - accuracy: 0.0108Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:45 - loss: 8.2967 - accuracy: 0.0128\n",
      "Epoch 00013: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 186s 123ms/step - loss: 8.3725 - accuracy: 0.0107 - val_loss: 8.2967 - val_accuracy: 0.0128\n",
      "Epoch 14/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 8.4034 - accuracy: 0.0114Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:48 - loss: 8.2905 - accuracy: 0.0120\n",
      "Epoch 00014: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 183s 121ms/step - loss: 8.4030 - accuracy: 0.0114 - val_loss: 8.2905 - val_accuracy: 0.0120\n",
      "Epoch 15/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 8.2729 - accuracy: 0.0110Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:41 - loss: 8.3115 - accuracy: 0.0133\n",
      "Epoch 00015: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 185s 122ms/step - loss: 8.2730 - accuracy: 0.0110 - val_loss: 8.3115 - val_accuracy: 0.0133\n",
      "Epoch 16/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 8.3482 - accuracy: 0.0117Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:49 - loss: 8.3810 - accuracy: 0.0134\n",
      "Epoch 00016: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 180s 119ms/step - loss: 8.3482 - accuracy: 0.0117 - val_loss: 8.3810 - val_accuracy: 0.0134\n",
      "Epoch 17/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: 8.3416 - accuracy: 0.0122Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:44 - loss: nan - accuracy: 0.0081\n",
      "Epoch 00017: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 187s 123ms/step - loss: 8.3422 - accuracy: 0.0122 - val_loss: nan - val_accuracy: 0.0081\n",
      "Epoch 18/30\n",
      "   5/1513 [..............................] - ETA: 56s - loss: nan - accuracy: 0.0063  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sebi\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py:1020: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1512/1513 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0081- E - ETA: 2s - loss: nan - accur - ETA: 1s - loss: nan - aEpoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:50 - loss: nan - accuracy: 0.0081- ETA:\n",
      "Epoch 00018: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 186s 123ms/step - loss: nan - accuracy: 0.0081 - val_loss: nan - val_accuracy: 0.0081\n",
      "Epoch 19/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0081- ETA: 1s - loss: nan - acEpoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:41 - loss: nan - accuracy: 0.0081\n",
      "Epoch 00019: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 184s 121ms/step - loss: nan - accuracy: 0.0081 - val_loss: nan - val_accuracy: 0.0081\n",
      "Epoch 20/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0081Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:46 - loss: nan - accuracy: 0.0081\n",
      "Epoch 00020: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 177s 117ms/step - loss: nan - accuracy: 0.0081 - val_loss: nan - val_accuracy: 0.0081\n",
      "Epoch 21/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0081- ETA: 0s - loss: nan - accuracy: 0.Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:43 - loss: nan - accuracy: 0.0081\n",
      "Epoch 00021: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 182s 120ms/step - loss: nan - accuracy: 0.0081 - val_loss: nan - val_accuracy: 0.0081\n",
      "Epoch 22/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0082Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:42 - loss: nan - accuracy: 0.0081\n",
      "Epoch 00022: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 188s 125ms/step - loss: nan - accuracy: 0.0082 - val_loss: nan - val_accuracy: 0.0081\n",
      "Epoch 23/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0080Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:42 - loss: nan - accuracy: 0.0081\n",
      "Epoch 00023: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 179s 118ms/step - loss: nan - accuracy: 0.0080 - val_loss: nan - val_accuracy: 0.0081\n",
      "Epoch 24/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0083Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:48 - loss: nan - accuracy: 0.0081\n",
      "Epoch 00024: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 178s 118ms/step - loss: nan - accuracy: 0.0083 - val_loss: nan - val_accuracy: 0.0081\n",
      "Epoch 25/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0079- ETA: 2s - loss: nEpoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:43 - loss: nan - accuracy: 0.0081- ETA: 1:45 - loss: nan\n",
      "Epoch 00025: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 184s 122ms/step - loss: nan - accuracy: 0.0079 - val_loss: nan - val_accuracy: 0.0081\n",
      "Epoch 26/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0083Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:43 - loss: nan - accuracy: 0.0081\n",
      "Epoch 00026: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 192s 127ms/step - loss: nan - accuracy: 0.0083 - val_loss: nan - val_accuracy: 0.0081\n",
      "Epoch 27/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0081Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:41 - loss: nan - accuracy: 0.0081\n",
      "Epoch 00027: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 176s 117ms/step - loss: nan - accuracy: 0.0081 - val_loss: nan - val_accuracy: 0.0081\n",
      "Epoch 28/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0082Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:48 - loss: nan - accuracy: 0.0081- ETA: 1:50 - loss\n",
      "Epoch 00028: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 178s 118ms/step - loss: nan - accuracy: 0.0082 - val_loss: nan - val_accuracy: 0.0081\n",
      "Epoch 29/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0081Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:42 - loss: nan - accuracy: 0.0081\n",
      "Epoch 00029: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 181s 120ms/step - loss: nan - accuracy: 0.0081 - val_loss: nan - val_accuracy: 0.0081\n",
      "Epoch 30/30\n",
      "1512/1513 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0082Epoch 1/30\n",
      " 377/1513 [======>.......................] - ETA: 1:42 - loss: nan - accuracy: 0.0081\n",
      "Epoch 00030: val_loss did not improve from 8.20651\n",
      "1513/1513 [==============================] - 177s 117ms/step - loss: nan - accuracy: 0.0082 - val_loss: nan - val_accuracy: 0.0081\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath = 'Bayesyian_CNN_Multiclass.hdf5', verbose = 1, save_best_only = True)\n",
    "\n",
    "modelBayes.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks = [checkpointer],\n",
    "    validation_steps=validation_generator.samples // batch_size)\n",
    "\n",
    "modelBayes.save_weights('Bayesyian_CNN_Multiclass-Final.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-7d18f4ae1225>:3: Model.evaluate_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.evaluate, which supports generators.\n",
      "WARNING:tensorflow:From <ipython-input-10-7d18f4ae1225>:4: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "     Apple Braeburn       0.01      0.01      0.01       164\n",
      " Apple Crimson Snow       0.01      0.01      0.01       148\n",
      "     Apple Golden 1       0.01      0.01      0.01       164\n",
      "     Apple Golden 2       0.01      0.01      0.01       164\n",
      "     Apple Golden 3       0.01      0.01      0.01       161\n",
      " Apple Granny Smith       0.01      0.01      0.01       164\n",
      "    Apple Pink Lady       0.01      0.01      0.01       152\n",
      "        Apple Red 1       0.00      0.00      0.00       164\n",
      "        Apple Red 2       0.01      0.01      0.01       164\n",
      "        Apple Red 3       0.01      0.01      0.01       144\n",
      "Apple Red Delicious       0.00      0.00      0.00       166\n",
      " Apple Red Yellow 1       0.01      0.01      0.01       164\n",
      " Apple Red Yellow 2       0.02      0.02      0.02       219\n",
      "            Apricot       0.00      0.00      0.00       164\n",
      "            Avocado       0.01      0.01      0.01       143\n",
      "       Avocado ripe       0.00      0.00      0.00       166\n",
      "             Banana       0.01      0.01      0.01       166\n",
      " Banana Lady Finger       0.00      0.00      0.00       152\n",
      "         Banana Red       0.03      0.02      0.02       166\n",
      "           Beetroot       0.00      0.00      0.00       150\n",
      "          Blueberry       0.02      0.02      0.02       154\n",
      "       Cactus fruit       0.00      0.00      0.00       166\n",
      "       Cantaloupe 1       0.00      0.00      0.00       164\n",
      "       Cantaloupe 2       0.02      0.02      0.02       164\n",
      "          Carambula       0.00      0.00      0.00       166\n",
      "        Cauliflower       0.01      0.01      0.01       234\n",
      "           Cherry 1       0.01      0.01      0.01       164\n",
      "           Cherry 2       0.02      0.02      0.02       246\n",
      "     Cherry Rainier       0.02      0.02      0.02       246\n",
      "   Cherry Wax Black       0.01      0.01      0.01       164\n",
      "     Cherry Wax Red       0.02      0.02      0.02       164\n",
      "  Cherry Wax Yellow       0.01      0.01      0.01       164\n",
      "           Chestnut       0.00      0.00      0.00       153\n",
      "         Clementine       0.01      0.01      0.01       166\n",
      "              Cocos       0.00      0.00      0.00       166\n",
      "              Dates       0.01      0.01      0.01       166\n",
      "           Eggplant       0.01      0.01      0.01       156\n",
      "        Ginger Root       0.02      0.02      0.02        99\n",
      "         Granadilla       0.01      0.01      0.01       166\n",
      "         Grape Blue       0.02      0.02      0.02       328\n",
      "         Grape Pink       0.00      0.00      0.00       164\n",
      "        Grape White       0.00      0.00      0.00       166\n",
      "      Grape White 2       0.01      0.01      0.01       166\n",
      "      Grape White 3       0.01      0.01      0.01       164\n",
      "      Grape White 4       0.01      0.01      0.01       158\n",
      "    Grapefruit Pink       0.01      0.01      0.01       166\n",
      "   Grapefruit White       0.00      0.00      0.00       164\n",
      "              Guava       0.01      0.01      0.01       166\n",
      "           Hazelnut       0.00      0.00      0.00       157\n",
      "        Huckleberry       0.02      0.02      0.02       166\n",
      "               Kaki       0.01      0.01      0.01       166\n",
      "               Kiwi       0.00      0.00      0.00       156\n",
      "           Kohlrabi       0.01      0.01      0.01       157\n",
      "           Kumquats       0.00      0.00      0.00       166\n",
      "              Lemon       0.00      0.00      0.00       164\n",
      "        Lemon Meyer       0.01      0.01      0.01       166\n",
      "              Limes       0.02      0.02      0.02       166\n",
      "             Lychee       0.01      0.01      0.01       166\n",
      "          Mandarine       0.00      0.00      0.00       166\n",
      "              Mango       0.00      0.00      0.00       166\n",
      "          Mango Red       0.01      0.01      0.01       142\n",
      "          Mangostan       0.01      0.01      0.01       102\n",
      "           Maracuja       0.02      0.02      0.02       166\n",
      " Melon Piel de Sapo       0.00      0.00      0.00       246\n",
      "           Mulberry       0.00      0.00      0.00       164\n",
      "          Nectarine       0.00      0.00      0.00       164\n",
      "     Nectarine Flat       0.01      0.01      0.01       160\n",
      "         Nut Forest       0.02      0.02      0.02       218\n",
      "          Nut Pecan       0.01      0.01      0.01       178\n",
      "          Onion Red       0.02      0.02      0.02       150\n",
      "   Onion Red Peeled       0.02      0.03      0.02       155\n",
      "        Onion White       0.01      0.01      0.01       146\n",
      "             Orange       0.00      0.00      0.00       160\n",
      "             Papaya       0.01      0.01      0.01       164\n",
      "      Passion Fruit       0.01      0.01      0.01       166\n",
      "              Peach       0.01      0.01      0.01       164\n",
      "            Peach 2       0.02      0.02      0.02       246\n",
      "         Peach Flat       0.01      0.01      0.01       164\n",
      "               Pear       0.01      0.01      0.01       164\n",
      "         Pear Abate       0.00      0.00      0.00       166\n",
      "       Pear Forelle       0.03      0.03      0.03       234\n",
      "        Pear Kaiser       0.01      0.01      0.01       102\n",
      "       Pear Monster       0.01      0.01      0.01       166\n",
      "           Pear Red       0.02      0.02      0.02       222\n",
      "      Pear Williams       0.00      0.00      0.00       166\n",
      "             Pepino       0.01      0.01      0.01       166\n",
      "       Pepper Green       0.01      0.01      0.01       148\n",
      "         Pepper Red       0.01      0.01      0.01       222\n",
      "      Pepper Yellow       0.02      0.02      0.02       222\n",
      "           Physalis       0.00      0.00      0.00       164\n",
      " Physalis with Husk       0.01      0.01      0.01       164\n",
      "          Pineapple       0.02      0.02      0.02       166\n",
      "     Pineapple Mini       0.00      0.00      0.00       163\n",
      "       Pitahaya Red       0.02      0.02      0.02       166\n",
      "               Plum       0.00      0.00      0.00       151\n",
      "             Plum 2       0.01      0.01      0.01       142\n",
      "             Plum 3       0.01      0.01      0.01       304\n",
      "        Pomegranate       0.01      0.01      0.01       164\n",
      "     Pomelo Sweetie       0.00      0.00      0.00       153\n",
      "         Potato Red       0.01      0.01      0.01       150\n",
      "  Potato Red Washed       0.01      0.01      0.01       151\n",
      "       Potato Sweet       0.01      0.01      0.01       150\n",
      "       Potato White       0.01      0.01      0.01       150\n",
      "             Quince       0.01      0.01      0.01       166\n",
      "           Rambutan       0.01      0.01      0.01       164\n",
      "          Raspberry       0.00      0.00      0.00       166\n",
      "         Redcurrant       0.00      0.00      0.00       164\n",
      "              Salak       0.00      0.00      0.00       162\n",
      "         Strawberry       0.00      0.00      0.00       164\n",
      "   Strawberry Wedge       0.02      0.02      0.02       246\n",
      "          Tamarillo       0.02      0.02      0.02       166\n",
      "            Tangelo       0.02      0.02      0.02       166\n",
      "           Tomato 1       0.02      0.02      0.02       246\n",
      "           Tomato 2       0.01      0.01      0.01       225\n",
      "           Tomato 3       0.00      0.00      0.00       246\n",
      "           Tomato 4       0.00      0.00      0.00       160\n",
      "  Tomato Cherry Red       0.00      0.00      0.00       164\n",
      "      Tomato Maroon       0.00      0.00      0.00       127\n",
      "      Tomato Yellow       0.01      0.01      0.01       153\n",
      "             Walnut       0.02      0.02      0.02       249\n",
      "\n",
      "           accuracy                           0.01     20622\n",
      "          macro avg       0.01      0.01      0.01     20622\n",
      "       weighted avg       0.01      0.01      0.01     20622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "test_steps_per_epoch = np.math.ceil(test_generator.samples / test_generator.batch_size)\n",
    "scores = model.evaluate_generator(test_generator)\n",
    "predictions = model.predict_generator(test_generator, steps=test_steps_per_epoch)\n",
    "# Get most likely class\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "true_classes = test_generator.classes\n",
    "class_labels = list(test_generator.class_indices.keys())  \n",
    "\n",
    "\n",
    "report = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
    "print(report) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "     Apple Braeburn       0.01      1.00      0.02       164\n",
      " Apple Crimson Snow       0.00      0.00      0.00       148\n",
      "     Apple Golden 1       0.00      0.00      0.00       164\n",
      "     Apple Golden 2       0.00      0.00      0.00       164\n",
      "     Apple Golden 3       0.00      0.00      0.00       161\n",
      " Apple Granny Smith       0.00      0.00      0.00       164\n",
      "    Apple Pink Lady       0.00      0.00      0.00       152\n",
      "        Apple Red 1       0.00      0.00      0.00       164\n",
      "        Apple Red 2       0.00      0.00      0.00       164\n",
      "        Apple Red 3       0.00      0.00      0.00       144\n",
      "Apple Red Delicious       0.00      0.00      0.00       166\n",
      " Apple Red Yellow 1       0.00      0.00      0.00       164\n",
      " Apple Red Yellow 2       0.00      0.00      0.00       219\n",
      "            Apricot       0.00      0.00      0.00       164\n",
      "            Avocado       0.00      0.00      0.00       143\n",
      "       Avocado ripe       0.00      0.00      0.00       166\n",
      "             Banana       0.00      0.00      0.00       166\n",
      " Banana Lady Finger       0.00      0.00      0.00       152\n",
      "         Banana Red       0.00      0.00      0.00       166\n",
      "           Beetroot       0.00      0.00      0.00       150\n",
      "          Blueberry       0.00      0.00      0.00       154\n",
      "       Cactus fruit       0.00      0.00      0.00       166\n",
      "       Cantaloupe 1       0.00      0.00      0.00       164\n",
      "       Cantaloupe 2       0.00      0.00      0.00       164\n",
      "          Carambula       0.00      0.00      0.00       166\n",
      "        Cauliflower       0.00      0.00      0.00       234\n",
      "           Cherry 1       0.00      0.00      0.00       164\n",
      "           Cherry 2       0.00      0.00      0.00       246\n",
      "     Cherry Rainier       0.00      0.00      0.00       246\n",
      "   Cherry Wax Black       0.00      0.00      0.00       164\n",
      "     Cherry Wax Red       0.00      0.00      0.00       164\n",
      "  Cherry Wax Yellow       0.00      0.00      0.00       164\n",
      "           Chestnut       0.00      0.00      0.00       153\n",
      "         Clementine       0.00      0.00      0.00       166\n",
      "              Cocos       0.00      0.00      0.00       166\n",
      "              Dates       0.00      0.00      0.00       166\n",
      "           Eggplant       0.00      0.00      0.00       156\n",
      "        Ginger Root       0.00      0.00      0.00        99\n",
      "         Granadilla       0.00      0.00      0.00       166\n",
      "         Grape Blue       0.00      0.00      0.00       328\n",
      "         Grape Pink       0.00      0.00      0.00       164\n",
      "        Grape White       0.00      0.00      0.00       166\n",
      "      Grape White 2       0.00      0.00      0.00       166\n",
      "      Grape White 3       0.00      0.00      0.00       164\n",
      "      Grape White 4       0.00      0.00      0.00       158\n",
      "    Grapefruit Pink       0.00      0.00      0.00       166\n",
      "   Grapefruit White       0.00      0.00      0.00       164\n",
      "              Guava       0.00      0.00      0.00       166\n",
      "           Hazelnut       0.00      0.00      0.00       157\n",
      "        Huckleberry       0.00      0.00      0.00       166\n",
      "               Kaki       0.00      0.00      0.00       166\n",
      "               Kiwi       0.00      0.00      0.00       156\n",
      "           Kohlrabi       0.00      0.00      0.00       157\n",
      "           Kumquats       0.00      0.00      0.00       166\n",
      "              Lemon       0.00      0.00      0.00       164\n",
      "        Lemon Meyer       0.00      0.00      0.00       166\n",
      "              Limes       0.00      0.00      0.00       166\n",
      "             Lychee       0.00      0.00      0.00       166\n",
      "          Mandarine       0.00      0.00      0.00       166\n",
      "              Mango       0.00      0.00      0.00       166\n",
      "          Mango Red       0.00      0.00      0.00       142\n",
      "          Mangostan       0.00      0.00      0.00       102\n",
      "           Maracuja       0.00      0.00      0.00       166\n",
      " Melon Piel de Sapo       0.00      0.00      0.00       246\n",
      "           Mulberry       0.00      0.00      0.00       164\n",
      "          Nectarine       0.00      0.00      0.00       164\n",
      "     Nectarine Flat       0.00      0.00      0.00       160\n",
      "         Nut Forest       0.00      0.00      0.00       218\n",
      "          Nut Pecan       0.00      0.00      0.00       178\n",
      "          Onion Red       0.00      0.00      0.00       150\n",
      "   Onion Red Peeled       0.00      0.00      0.00       155\n",
      "        Onion White       0.00      0.00      0.00       146\n",
      "             Orange       0.00      0.00      0.00       160\n",
      "             Papaya       0.00      0.00      0.00       164\n",
      "      Passion Fruit       0.00      0.00      0.00       166\n",
      "              Peach       0.00      0.00      0.00       164\n",
      "            Peach 2       0.00      0.00      0.00       246\n",
      "         Peach Flat       0.00      0.00      0.00       164\n",
      "               Pear       0.00      0.00      0.00       164\n",
      "         Pear Abate       0.00      0.00      0.00       166\n",
      "       Pear Forelle       0.00      0.00      0.00       234\n",
      "        Pear Kaiser       0.00      0.00      0.00       102\n",
      "       Pear Monster       0.00      0.00      0.00       166\n",
      "           Pear Red       0.00      0.00      0.00       222\n",
      "      Pear Williams       0.00      0.00      0.00       166\n",
      "             Pepino       0.00      0.00      0.00       166\n",
      "       Pepper Green       0.00      0.00      0.00       148\n",
      "         Pepper Red       0.00      0.00      0.00       222\n",
      "      Pepper Yellow       0.00      0.00      0.00       222\n",
      "           Physalis       0.00      0.00      0.00       164\n",
      " Physalis with Husk       0.00      0.00      0.00       164\n",
      "          Pineapple       0.00      0.00      0.00       166\n",
      "     Pineapple Mini       0.00      0.00      0.00       163\n",
      "       Pitahaya Red       0.00      0.00      0.00       166\n",
      "               Plum       0.00      0.00      0.00       151\n",
      "             Plum 2       0.00      0.00      0.00       142\n",
      "             Plum 3       0.00      0.00      0.00       304\n",
      "        Pomegranate       0.00      0.00      0.00       164\n",
      "     Pomelo Sweetie       0.00      0.00      0.00       153\n",
      "         Potato Red       0.00      0.00      0.00       150\n",
      "  Potato Red Washed       0.00      0.00      0.00       151\n",
      "       Potato Sweet       0.00      0.00      0.00       150\n",
      "       Potato White       0.00      0.00      0.00       150\n",
      "             Quince       0.00      0.00      0.00       166\n",
      "           Rambutan       0.00      0.00      0.00       164\n",
      "          Raspberry       0.00      0.00      0.00       166\n",
      "         Redcurrant       0.00      0.00      0.00       164\n",
      "              Salak       0.00      0.00      0.00       162\n",
      "         Strawberry       0.00      0.00      0.00       164\n",
      "   Strawberry Wedge       0.00      0.00      0.00       246\n",
      "          Tamarillo       0.00      0.00      0.00       166\n",
      "            Tangelo       0.00      0.00      0.00       166\n",
      "           Tomato 1       0.00      0.00      0.00       246\n",
      "           Tomato 2       0.00      0.00      0.00       225\n",
      "           Tomato 3       0.00      0.00      0.00       246\n",
      "           Tomato 4       0.00      0.00      0.00       160\n",
      "  Tomato Cherry Red       0.00      0.00      0.00       164\n",
      "      Tomato Maroon       0.00      0.00      0.00       127\n",
      "      Tomato Yellow       0.00      0.00      0.00       153\n",
      "             Walnut       0.00      0.00      0.00       249\n",
      "\n",
      "           accuracy                           0.01     20622\n",
      "          macro avg       0.00      0.01      0.00     20622\n",
      "       weighted avg       0.00      0.01      0.00     20622\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sebi\\.conda\\envs\\keras-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# evaluate the modelBayes\n",
    "test_steps_per_epoch = np.math.ceil(test_generator.samples / test_generator.batch_size)\n",
    "scores = modelBayes.evaluate_generator(test_generator)\n",
    "predictions = modelBayes.predict_generator(test_generator, steps=test_steps_per_epoch)\n",
    "# Get most likely class\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "true_classes = test_generator.classes\n",
    "class_labels = list(test_generator.class_indices.keys())  \n",
    "\n",
    "\n",
    "report = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
    "print(report) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
